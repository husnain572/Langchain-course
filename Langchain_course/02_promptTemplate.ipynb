{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "122a0eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "_=load_dotenv(find_dotenv())\n",
    "GROQ_API_KEY=os.environ['GROQ_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2077f540",
   "metadata": {},
   "source": [
    "### Completion model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e767382c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9d12b76c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='It was a chilly winter evening in 1895 when Nikola Tesla, the renowned inventor and engineer, arrived at his laboratory in New York City. As he began to work on his latest experiment, he noticed something peculiar. His equipment, which included a variety of electrical devices and machinery, seemed to be behaving erratically.\\n\\nTesla, being a man of immense curiosity and dedication to his work, decided to investigate the cause of the disturbance. He spent hours checking and rechecking his equipment, but to no avail. The machines continued to malfunction, and Tesla was at a loss for an explanation.\\n\\nJust as he was about to give up for the night, Tesla received an unexpected visit from his friend and fellow inventor, George Westinghouse. Westinghouse had stopped by to discuss some business matters, but as he entered the laboratory, he noticed the strange occurrences.\\n\\n\"Tesla, what\\'s going on here?\" Westinghouse asked, eyeing the malfunctioning equipment. \"It looks like your machines are possessed or something!\"\\n\\nTesla chuckled and replied, \"I know, it\\'s as if they\\'re being controlled by an invisible force. I\\'ve never seen anything like it.\"\\n\\nIntrigued, Westinghouse suggested that they try to communicate with the unknown force. Tesla, being a man of science, was skeptical, but he decided to humor his friend. Together, they began to ask questions, hoping to receive some kind of response.\\n\\nTo their astonishment, the machines began to respond. They would turn on and off, and even change their settings, in apparent answer to the questions being asked. Tesla and Westinghouse were amazed, and their curiosity was piqued.\\n\\nAs the night wore on, they continued to experiment, trying to understand the nature of the mysterious force. They soon realized that the disturbance was not limited to the laboratory; it seemed to be affecting the entire city\\'s electrical grid.\\n\\nTesla, being a pioneer in the field of electrical engineering, had a theory. He believed that the force was not supernatural, but rather a manifestation of the Earth\\'s own electromagnetic field. He proposed that the planet was, in fact, a giant resonator, and that his equipment was simply tapping into this energy.\\n\\nWestinghouse was fascinated by the idea and encouraged Tesla to pursue it further. And so, Tesla dedicated himself to studying the Earth\\'s electromagnetic field, which he believed held the key to unlocking the secrets of the universe.\\n\\nThe strange occurrences in Tesla\\'s laboratory that night marked the beginning of a new era in his research. He went on to develop groundbreaking theories and technologies, including the concept of resonance and the development of alternating current (AC) systems.\\n\\nThough the exact nature of the mysterious force remains a mystery to this day, Tesla\\'s work on the Earth\\'s electromagnetic field paved the way for numerous scientific breakthroughs and continues to inspire new generations of inventors and researchers. The curious story of that winter evening in 1895 serves as a testament to the power of human curiosity and the boundless potential of the human mind.' response_metadata={'token_usage': {'completion_tokens': 612, 'prompt_tokens': 42, 'total_tokens': 654, 'completion_time': 1.713600322, 'prompt_time': 0.001900053, 'queue_time': 0.052498857, 'total_time': 1.715500375}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_3f3b593e33', 'finish_reason': 'stop', 'logprobs': None} id='run-37013122-39e4-468a-88b2-566f5d7c7b24-0' usage_metadata={'input_tokens': 42, 'output_tokens': 612, 'total_tokens': 654}\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "llmModel = ChatGroq(model=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "# This is for completions model\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"Tell me a {adjective} story about {topic}\"\n",
    ")\n",
    "\n",
    "llmModelPrompt = prompt_template.format(\n",
    "    adjective=\"curious\",\n",
    "    topic=\"Tesla\"\n",
    ")\n",
    "\n",
    "res = llmModel.invoke(llmModelPrompt)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb357167",
   "metadata": {},
   "source": [
    "## chat completion model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b2bd7836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joseph P. Kennedy, the patriarch of the Kennedy family, had a total of 9 grandchildren.\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "chatModel=ChatGroq(model='llama-3.1-8b-instant')\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "chat_template=ChatPromptTemplate.from_messages(\n",
    "  [  \n",
    "      ('system', 'You are an {profession} expert on {topic}'),\n",
    "    ('human', 'Hello, Mr. {profession}, can you please answer a question?'),\n",
    "    ('ai', 'Sure!'),\n",
    "    ('human', '{user_input}')\n",
    "    ]\n",
    ")\n",
    "messages=chat_template.format_messages(\n",
    "    profession='Historian',\n",
    "    topic='The Kennedy Family',\n",
    "    user_input='How many grandchildren had Joseph P. Kennedy?'\n",
    ")\n",
    "response=chatModel.invoke(messages) \n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabc804f",
   "metadata": {},
   "source": [
    "## Few Shot Prompting & Chains\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e92dd18a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estoy bien, gracias. ¿Y tú? (I'm fine, thanks. And you?)\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import FewShotChatMessagePromptTemplate\n",
    "\n",
    "examples = [\n",
    "    {\"input\": \"hi!\", \"output\": \"¡hola!\"},\n",
    "    {\"input\": \"bye!\", \"output\": \"¡adiós!\"},\n",
    "]\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an English-Spanish translator.\"),\n",
    "        few_shot_prompt,\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain=final_prompt | chatModel\n",
    "res=chain.invoke({\"input\": \"How are you?\"})\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9036dc74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Estoy bien, gracias. ¿Y tú? (I'm fine, thanks. And you?)\", response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 80, 'total_tokens': 101, 'completion_time': 0.020541844, 'prompt_time': 0.004267606, 'queue_time': 0.052553834, 'total_time': 0.02480945}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_7b3cfae3af', 'finish_reason': 'stop', 'logprobs': None}, id='run-e99872ed-b0d4-4352-8c7f-fafc207244ac-0', usage_metadata={'input_tokens': 80, 'output_tokens': 21, 'total_tokens': 101})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2e5b47",
   "metadata": {},
   "source": [
    "## Output Parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5f81f2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "llmModel = ChatGroq(model=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "# This is for completions model\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.output_parsers.json import SimpleJsonOutputParser\n",
    "\n",
    "json_prompt=PromptTemplate.from_template(\n",
    "        \"Return a JSON object with an `answer` key that answers the following question: {question}\"\n",
    ")\n",
    "json_parser=SimpleJsonOutputParser()\n",
    "json_chain=json_prompt | llmModel | json_parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "31aa6d5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Return a JSON object.'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "046dbd7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'Russia'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res=json_chain.invoke({\"question\": \"What is the biggest country?\"})\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4bca3950",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ee8c82",
   "metadata": {},
   "source": [
    "Optionally, you can use Pydantic to define a custom output format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3ad85457",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "70383395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Pydantic Object with the desired output format.\n",
    "class Joke(BaseModel):\n",
    "    setup:str = Field(description=\"Question to set up a joke.\")\n",
    "    punchline:str=Field(description=\"answer to resolve the joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9b6c4fd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'setup': \"Why couldn't the bicycle stand up by itself?\",\n",
       " 'punchline': 'Because it was two-tired.'}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = JsonOutputParser(pydantic_object=Joke)\n",
    "\n",
    "# Add the parser format instructions in the prompt definition.\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "# Create a chain with the prompt and the parser\n",
    "chain = prompt | chatModel | parser\n",
    "\n",
    "chain.invoke({\"query\": \"Tell me a joke.\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
